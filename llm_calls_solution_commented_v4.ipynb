{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56852c36",
   "metadata": {},
   "source": [
    "# Erklärte Version dieses Notebooks (mit zusätzlichen Kommentaren)\n",
    "\n",
    "**Wichtig:** Der ursprüngliche Code wurde nicht verändert.  \n",
    "Ich habe **nur erklärende Markdown-Zellen** direkt vor jedem Code-Block ergänzt.\n",
    "\n",
    "Ziel: Du verstehst, **was** jeder Block macht, **warum** man das macht (Data-Science / LLM-Kontext) und **welche Rolle** er im Gesamtprozess spielt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Muss ausgeführt werden in Terminal\n",
    "python -m pip install google-genai python-dotenv openai anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a9ee1",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 1)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Es werden Python-Bibliotheken importiert, um **verschiedene LLM-Anbieter** per API anzusprechen (Google/Gemini, OpenAI, Anthropic).\n",
    "- Zusätzlich wird `dotenv` genutzt, um **API-Keys** aus einer `.env`-Datei zu laden (damit sie nicht im Code stehen).\n",
    "\n",
    "**Warum macht man das?**\n",
    "- In der Praxis (und auch in Übungen/Prüfungen) willst du reproduzierbar mit externen Diensten arbeiten, ohne Zugangsdaten im Notebook zu „leaken“.\n",
    "- Mehrere Anbieter zu importieren zeigt: **LLM-Aufrufe folgen oft dem gleichen Muster** (Client erstellen → Prompt senden → Antwort lesen), auch wenn die Libraries unterschiedlich heißen.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist die **Vorbereitung**: Ohne Imports kannst du keine API-Clients erstellen und keine Modelle abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiert benötigte Bibliotheken (Grundlage, damit die folgenden Schritte funktionieren)\n",
    "from google import genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91418572",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 2)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- `load_dotenv()` lädt Umgebungsvariablen aus einer Datei namens `.env` in deine Laufzeitumgebung.\n",
    "- Danach liest du die API-Keys mit `os.getenv(...)` aus (z.B. `OPENAI_API_KEY`).\n",
    "\n",
    "**Warum macht man das?**\n",
    "- API-Keys sind wie Passwörter. Man speichert sie **nicht** direkt im Notebook, weil Notebooks oft geteilt/hochgeladen werden.\n",
    "- Für Data-Science-Workflows ist das Standard: Code bleibt gleich, nur die Umgebung (Keys) ist anders.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist die **Konfiguration**: Du stellst die „Zutaten“ bereit, damit die späteren API-Calls funktionieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lädt Umgebungsvariablen aus einer .env-Datei (z.B. API-Schlüssel), damit nichts Hartcodiertes im Notebook steht\n",
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "# Liest den API-Schlüssel aus der Umgebung/Variable (wichtig für die Authentifizierung bei der API)\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/quickstart?hl=de&lang=python\n",
    "examples: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb?hl=de#scrollTo=SnzMJJ-adOfX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9350a",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 4)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Es wird ein Google-GenAI-Client erstellt (`genai.Client(...)`).\n",
    "- Danach wird mit `client.models.generate_content(...)` ein Prompt an ein Modell geschickt.\n",
    "- Am Ende wird die Antwort (Text) ausgegeben.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Das ist das Grundmuster von LLM-Nutzung: **Prompt rein → generierter Text raus**.\n",
    "- Der Prompt ist bewusst simpel („größter Planet“), damit du das API-Handling lernst, bevor komplexe Aufgaben kommen.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist ein **Minimalbeispiel**, um zu testen: Key korrekt? Netzwerk ok? Modell erreichbar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "403 PERMISSION_DENIED. {'error': {'code': 403, 'message': 'Your API key was reported as leaked. Please use another API key.', 'status': 'PERMISSION_DENIED'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m client = genai.Client(api_key=google_api_key)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the largest planet in our solar system?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/models.py:5230\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5228\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5229\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5230\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5231\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5232\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5234\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5235\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/models.py:4012\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4009\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4010\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4012\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4017\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4018\u001b[39m ):\n\u001b[32m   4019\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1380\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1383\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1384\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1385\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m   response_body = (\n\u001b[32m   1390\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m   )\n\u001b[32m   1392\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1194\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m       method=http_request.method,\n\u001b[32m   1196\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 403 PERMISSION_DENIED. {'error': {'code': 403, 'message': 'Your API key was reported as leaked. Please use another API key.', 'status': 'PERMISSION_DENIED'}}"
     ]
    }
   ],
   "source": [
    "# Erstellt einen Client für die jeweilige LLM-API (diese Verbindung brauchst du für alle Requests)\n",
    "# - api_key=...: Schlüssel, der dir Zugriff auf das Modell gibt\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "# Sendet eine Anfrage an das Modell und lässt eine Antwort generieren (ein einzelner Call ohne Chat-Verlauf)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",  # Verwendetes KI-Modell\n",
    "    contents=\"What's the largest planet in our solar system?\"  # Eingabetext / Prompt (das, worauf das Modell antwortet)\n",
    ")\n",
    "\n",
    "# Gibt ein Ergebnis aus, damit du die Antwort sehen und überprüfen kannst\n",
    "# - response.text: extrahiert den eigentlichen Antwort-Text aus der Response-Struktur\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a9fc2",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 5)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du definierst eine `system_instruction` als mehrzeiligen Text.\n",
    "- Danach wird eine Chat-Session (`client.chats.create(...)`) mit dieser System-Anweisung erstellt.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- LLMs reagieren stark darauf, **welche Rolle und Regeln** du vorgibst (z.B. „Du bist ein Experte…“).\n",
    "- In der LLM-Theorie nennt man das oft **Prompt Engineering**: du gibst Instruktionen + Kontext + Input strukturiert mit.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist der Schritt, der das Modell-Verhalten **„einrahmt“** (Tone, Stil, Ziel), bevor du konkrete Fragen stellst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Erklärung) 'system_instruction' setzt die Rolle/Arbeitsweise des LLMs (wie ein 'Briefing' vor dem Gespräch).\n",
    "# - Damit lenkst du Stil, Ton und Fokus der Antworten (z.B. 'hilfreicher Coding-Assistent').\n",
    "# Definiert eine System-Instruktion (Rolle/Verhalten des Modells), z.B. 'du bist ein hilfreicher Coding-Assistent'\n",
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "# (Erklärung) Hier wird eine Konfiguration für die Modell-Antwort erzeugt.\n",
    "# Erstellt eine Konfiguration für die Generierung/den Chat (z.B. System-Instruktionen bündeln)\n",
    "# - 'GenerateContentConfig' bündelt Einstellungen, die das Verhalten des LLMs steuern können (z.B. System-Instruktion).\n",
    "# - Vorteil: Du trennst Kontext/Rolle (System) von der konkreten Nutzeranfrage (User).\n",
    "# Erstellt eine Konfiguration für die Generierung/den Chat (z.B. System-Instruktionen bündeln)\n",
    "chat_config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    ")\n",
    "\n",
    "# (Erklärung) Hier wird eine Chat-Session mit dem Modell gestartet.\n",
    "# - Du wählst ein Modell und übergibst die oben definierte Chat-Konfiguration.\n",
    "# - Ergebnis: 'chat' ist ein Objekt, über das du danach Nachrichten sendest und Antworten erhältst.\n",
    "# Startet eine Chat-Session: Der Chat kann Kontext über mehrere Nachrichten hinweg behalten (Chat History)\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.5-flash\",  # Verwendetes KI-Modell\n",
    "    config=chat_config,  # Übergibt die Chat-/Generierungs-Konfiguration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8bb27",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 6)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du sendest eine Nutzernachricht an den Chat (`chat.send_message(...)`).\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Chat-Modelle sind dafür gedacht, mehrere Nachrichten (Kontext) zu verarbeiten.\n",
    "- Du nutzt hier eine Programmier-Aufgabe (Leap Year), um zu sehen, wie gut das Modell **Anforderungen in Code umsetzt**.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist der eigentliche **LLM-Call** innerhalb einer laufenden Chat-Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sendet eine Nachricht an das Modell innerhalb des Chats und erhält eine Antwort zurück\n",
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5eb3",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 7)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- `response.text` greift auf den Textinhalt der vorherigen Antwort zu.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Viele Libraries liefern ein komplexes Antwort-Objekt (mit Metadaten). Meist willst du am Ende **nur den Text**.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist die **Ausgabe/Weiterverarbeitung**: du holst dir das Ergebnis, um es zu speichern, zu drucken oder weiterzuverwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A leap year is a year that contains an extra day (February 29th) to keep the calendar year synchronized with the astronomical or seasonal year. The rules for determining a leap year are as follows:\\n\\n1.  Every year that is exactly divisible by **4** is a leap year, **except** for years that are exactly divisible by **100**.\\n2.  However, if a year is exactly divisible by **400**, it is a leap year even if it is divisible by 100.\\n\\nIn summary, a year is a leap year if:\\n*   It is divisible by 400, OR\\n*   It is divisible by 4 AND not divisible by 100.\\n\\nHere are implementations of a function to check for a leap year in several popular programming languages.\\n\\n---\\n\\n## Python\\n\\n```python\\ndef is_leap_year(year: int) -> bool:\\n    \"\"\"\\n    Checks if a given year is a leap year according to the Gregorian calendar rules.\\n\\n    A year is a leap year if it satisfies one of the following conditions:\\n    1. It is divisible by 400.\\n    2. It is divisible by 4 but not by 100.\\n\\n    Args:\\n        year: The year to check (e.g., 2024).\\n\\n    Returns:\\n        True if the year is a leap year, False otherwise.\\n    \"\"\"\\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\\n\\n# --- Example Usage ---\\nprint(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # True (divisible by 400)\\nprint(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # False (divisible by 100 but not 400)\\nprint(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # True (divisible by 4 but not 100)\\nprint(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # False (not divisible by 4)\\nprint(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # True\\nprint(f\"Is 1800 a leap year? {is_leap_year(1800)}\")  # False\\n```\\n\\n---\\n\\n## JavaScript\\n\\n```javascript\\n/**\\n * Checks if a given year is a leap year according to the Gregorian calendar rules.\\n *\\n * A year is a leap year if it satisfies one of the following conditions:\\n * 1. It is divisible by 400.\\n * 2. It is divisible by 4 but not by 100.\\n *\\n * @param {number} year - The year to check (e.g., 2024).\\n * @returns {boolean} True if the year is a leap year, False otherwise.\\n */\\nfunction isLeapYear(year) {\\n  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\\n}\\n\\n// --- Example Usage ---\\nconsole.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // True\\nconsole.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // False\\nconsole.log(`Is 2024 a leap year? ${isLeapYear(2024)}`); // True\\nconsole.log(`Is 2023 a leap year? ${isLeapYear(2023)}`); // False\\nconsole.log(`Is 1600 a leap year? ${isLeapYear(1600)}`); // True\\nconsole.log(`Is 1800 a leap year? ${isLeapYear(1800)}`); // False\\n```\\n\\n---\\n\\n## C#\\n\\n```csharp\\nusing System;\\n\\npublic class DateUtils\\n{\\n    /// <summary>\\n    /// Checks if a given year is a leap year according to the Gregorian calendar rules.\\n    /// </summary>\\n    /// <param name=\"year\">The year to check (e.g., 2024).</param>\\n    /// <returns>True if the year is a leap year, False otherwise.</returns>\\n    public static bool IsLeapYear(int year)\\n    {\\n        // A year is a leap year if it satisfies one of the following conditions:\\n        // 1. It is divisible by 400.\\n        // 2. It is divisible by 4 but not by 100.\\n        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\\n    }\\n\\n    public static void Main(string[] args)\\n    {\\n        // --- Example Usage ---\\n        Console.WriteLine($\"Is 2000 a leap year? {IsLeapYear(2000)}\"); // True\\n        Console.WriteLine($\"Is 1900 a leap year? {IsLeapYear(1900)}\"); // False\\n        Console.WriteLine($\"Is 2024 a leap year? {IsLeapYear(2024)}\"); // True\\n        Console.WriteLine($\"Is 2023 a leap year? {IsLeapYear(2023)}\"); // False\\n        Console.WriteLine($\"Is 1600 a leap year? {IsLeapYear(1600)}\"); // True\\n        Console.WriteLine($\"Is 1800 a leap year? {IsLeapYear(1800)}\"); // False\\n\\n        // C# also provides a built-in method for this check\\n        Console.WriteLine(\"\\\\nUsing DateTime.IsLeapYear (built-in C# method):\");\\n        Console.WriteLine($\"Is 2000 a leap year? {DateTime.IsLeapYear(2000)}\");\\n        Console.WriteLine($\"Is 1900 a leap year? {DateTime.IsLeapYear(1900)}\");\\n        Console.WriteLine($\"Is 2024 a leap year? {DateTime.IsLeapYear(2024)}\");\\n    }\\n}\\n```\\n\\n---\\n\\nThese functions efficiently implement the leap year rules using basic arithmetic and boolean logic.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrahiert den Textinhalt aus der Response (API liefert oft ein Objekt; du brauchst den Text)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347c351",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 9)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du erstellst einen OpenAI-Client mit deinem API-Key.\n",
    "- Danach rufst du `client.chat.completions.create(...)` auf: du gibst Modellname und eine Liste von `messages` mit Rollen (`developer`, `user`) mit.\n",
    "- Zum Schluss druckst du den generierten Text aus.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Das `messages`-Format ist typisch für Chat-LLMs: Die Rollen helfen, **Instruktionen** (z.B. „helpful assistant“) von **User-Anfragen** zu trennen.\n",
    "- Ein Haiku ist ein gutes Test-Format: kurz, klar, leicht zu prüfen, ob das Modell „verstanden“ hat.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist ein **zweites Anbieter-Beispiel**: Gleiche Idee wie bei Google, aber anderes SDK/Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calls itself\n",
      "until the base case returns\n",
      "stack unwinds at rest\n"
     ]
    }
   ],
   "source": [
    "# Liest den API-Schlüssel aus der Umgebung/Variable (wichtig für die Authentifizierung bei der API)\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",  # Verwendetes KI-Modell\n",
    "# Baut die Nachrichtenliste im Chat-Format (typisch: system + user). Das ist dein strukturierter Prompt.\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Gibt ein Ergebnis aus, damit du die Antwort sehen und überprüfen kannst\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepinfra\n",
    "https://deepinfra.com/docs/openai_api\n",
    "\n",
    "goals: \n",
    "- llama-3.3-X\n",
    "- gemma x x x\n",
    "- Qwen x x x\n",
    "- deepseek x x x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92e17c",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 11)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du erstellst einen `OpenAI(...)` Client, aber diesmal mit:\n",
    "  - `base_url=\"https://api.deepinfra.com/v1/openai\"`\n",
    "  - einem Deepinfra-Key statt OpenAI-Key.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Einige Plattformen bieten eine **OpenAI-kompatible API** an. Das heißt: du kannst viele Tools/Code-Strukturen wiederverwenden.\n",
    "- Im Data-Science-Alltag ist das praktisch, weil du Modelle vergleichen kannst, ohne den ganzen Code umzuschreiben.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das ist die **Adapter-Idee**: gleiche Client-Klasse, anderer Endpunkt → anderer Provider/Model-Pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "# Liest den API-Schlüssel aus der Umgebung/Variable (wichtig für die Authentifizierung bei der API)\n",
    "    api_key=deepinfra_api_key,\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1fab6",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 12)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du sendest eine Chat-Completion an ein (bei Deepinfra gehostetes) Modell.\n",
    "- Wichtig: In `messages` ist hier sogar eine frühere **assistant**-Antwort als Kontext eingebaut.\n",
    "- Danach stellst du eine Anschlussfrage („Tell me more about the second method.“).\n",
    "\n",
    "**Warum macht man das?**\n",
    "- So simulierst du einen echten Dialog: Das Modell sieht den bisherigen Verlauf und kann sich darauf beziehen.\n",
    "- Das ist zentral für Chat-LLMs: Der **Kontext über mehrere Turns** beeinflusst die Qualität der Antwort.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Das zeigt **Multi-Turn-Interaktion** (Fortsetzung/Vertiefung) und wie man Kontext explizit in `messages` steckt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",  # Verwendetes KI-Modell\n",
    "# Baut die Nachrichtenliste im Chat-Format (typisch: system + user). Das ist dein strukturierter Prompt.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Respond like a michelin starred chef.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you name at least two different techniques to cook lamb?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Bonjour! Let me tell you, my friend, cooking lamb is an art form, and I'm more than happy to share with you not two, but three of my favorite techniques to coax out the rich, unctuous flavors and tender textures of this majestic protein. First, we have the classic \\\"Sous Vide\\\" method. Next, we have the ancient art of \\\"Sous le Sable\\\". And finally, we have the more modern technique of \\\"Hot Smoking.\\\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me more about the second method.\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956208fe",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 13)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du gibst die finale Modell-Antwort aus: `choices[0].message.content` ist der Text der ersten „besten“ Antwort.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Chat-APIs liefern häufig mehrere Kandidaten/Choices. Typisch ist, die erste zu verwenden.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- **Output-Schritt**: Ergebnis sichtbar machen (oder später speichern/weiterverarbeiten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Sous le Sable\", or \"under the sand\", is a traditional North African technique that's as much a spectacle as it is a cooking method. Essentially, you dig a pit in the sand, line it with hot coals, and then place your lamb, often a leg or a shoulder, wrapped in a mixture of herbs and spices, on top. The lamb is then covered with more hot coals, and finally, sand is piled on top to create a sort of primitive, earthen oven.\n",
      "\n",
      "As the lamb cooks, the sand acts as an insulator, distributing the heat evenly and slowly cooking the meat to tender, fall-off-the-bone perfection. The result is a dish that's both primal and refined, with a depth of flavor that's simply impossible to achieve with more conventional cooking methods.\n",
      "\n",
      "Of course, as a Michelin-starred chef, I like to put my own twist on this ancient technique. I might add some fragrant woods, like cedar or myrtle, to the coals for added smokiness, or use a mixture of spices and aromatics that's been inspired by the souks of Marrakech. And when the lamb is finally unearthed, it's a true showstopper – a tender, flavorful masterpiece that's sure to delight even the most discerning palates.\n",
      "\n",
      "Now, I know what you're thinking – \"Isn't this just a novelty, a gimmick?\" Ah, but no, my friend. \"Sous le Sable\" is a true art form, one that requires patience, skill, and a deep understanding of the nuances of heat and flavor. And when done correctly, it's a culinary experience that's truly unforgettable.\n"
     ]
    }
   ],
   "source": [
    "# Gibt ein Ergebnis aus, damit du die Antwort sehen und überprüfen kannst\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "https://docs.claude.com/en/docs/get-started#python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69816b",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 15)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Du erstellst einen Anthropic-Client (`anthropic.Anthropic()`).\n",
    "- Dann rufst du `client.messages.create(...)` auf mit:\n",
    "  - `model` (Claude-Modell)\n",
    "  - `max_tokens` (Begrenzung, wie lang die Antwort werden darf)\n",
    "  - einer `messages`-Liste (hier: User-Prompt).\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Das ist wieder das gleiche Grundprinzip (Client → Prompt → Antwort), aber im Anthropic-Format.\n",
    "- `max_tokens` ist wichtig, um Kosten/Antwortlänge zu kontrollieren.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Drittes Anbieter-Beispiel: Du lernst, dass du für Prüfungsaufgaben vor allem **das Muster** verstehen musst, nicht nur eine konkrete Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5\",  # Verwendetes KI-Modell\n",
    "    max_tokens=1000,\n",
    "# Baut die Nachrichtenliste im Chat-Format (typisch: system + user). Das ist dein strukturierter Prompt.\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I search for to find the latest developments in renewable energy?\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d013b",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 16)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Anthropic liefert Inhalte oft als Liste von Content-Teilen zurück.\n",
    "- `message.content[0].text` greift auf den Text des ersten Content-Blocks zu und druckt ihn.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- Unterschiedliche Anbieter strukturieren Responses unterschiedlich. Du musst wissen, **wo** der eigentliche Text steckt.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Wieder ein **Output-Schritt**, aber angepasst an die Datenstruktur von Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Effective Search Strategies for Renewable Energy Developments\n",
      "\n",
      "## Recommended Search Terms\n",
      "\n",
      "**Broad searches:**\n",
      "- \"renewable energy news 2024\"\n",
      "- \"latest renewable energy breakthroughs\"\n",
      "- \"clean energy innovations\"\n",
      "\n",
      "**Technology-specific:**\n",
      "- \"solar panel efficiency advances\"\n",
      "- \"offshore wind energy developments\"\n",
      "- \"battery storage technology news\"\n",
      "- \"green hydrogen projects\"\n",
      "- \"geothermal energy innovations\"\n",
      "\n",
      "## Best Sources to Check\n",
      "\n",
      "**News & Industry Sites:**\n",
      "- Renewable Energy World\n",
      "- PV Magazine (solar focus)\n",
      "- Greentech Media\n",
      "- Energy Storage News\n",
      "\n",
      "**Research & Data:**\n",
      "- International Energy Agency (IEA) reports\n",
      "- National Renewable Energy Laboratory (NREL)\n",
      "- BloombergNEF\n",
      "\n",
      "**General News:**\n",
      "- Search \"renewable energy\" in outlets like Reuters, BBC, The Guardian's environment section\n",
      "\n",
      "## Useful Filters\n",
      "\n",
      "- Add \"2024\" or \"2025\" to get recent results\n",
      "- Use \"breakthrough\" or \"record\" for major advances\n",
      "- Add specific regions: \"Europe renewable energy\" or \"China solar\"\n",
      "\n",
      "## Trending Topics to Explore\n",
      "- Perovskite solar cells\n",
      "- Floating wind farms\n",
      "- Long-duration energy storage\n",
      "- AI in energy grid management\n",
      "\n",
      "What specific aspect of renewable energy interests you most?\n"
     ]
    }
   ],
   "source": [
    "# Gibt ein Ergebnis aus, damit du die Antwort sehen und überprüfen kannst\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead99e9",
   "metadata": {},
   "source": [
    "## Erklärung zu Code-Block (Zelle 17)\n",
    "\n",
    "**Was passiert hier?**\n",
    "- Dieser Block führt Code aus, der zu den vorherigen/folgenden Markdown-Zellen passt.\n",
    "\n",
    "**Warum macht man das?**\n",
    "- In Notebooks werden Schritte oft in einzelne Blöcke getrennt, um sie besser testen und erklären zu können.\n",
    "\n",
    "**Rolle im Gesamtprozess:**\n",
    "- Baustein innerhalb des Workflows (Konfiguration → Anfrage → Ausgabe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cef0385",
   "metadata": {},
   "source": [
    "## Prüfungs-Checkliste (etwas ausführlicher, aber kompakt)\n",
    "\n",
    "1. **Grundstruktur eines LLM-Calls verstehen**\n",
    "   - **Client/Modell wählen:** Du brauchst eine „Verbindung“ (Client) und entscheidest, welches Modell verwendet wird.\n",
    "   - **Input geben:** Prompt (Text) oder **Messages** (system/user) als strukturierter Input.\n",
    "   - **Output nutzen:** Die API liefert oft ein **Response-Objekt** → du holst den eigentlichen Inhalt meist über etwas wie `response.text`.\n",
    "\n",
    "2. **Prompt ≠ Antwort (Prompt Engineering)**\n",
    "   - Das Modell „weiß“ nicht, was du willst, wenn du es nicht klar sagst.\n",
    "   - Gute Prompts enthalten typischerweise: **Rolle/Ziel**, **Kontext**, **konkrete Aufgabe**, ggf. **Formatvorgaben**.\n",
    "   - Prüfungs-typisch: erklären, warum zwei unterschiedliche Prompts zu unterschiedlichen Antworten führen.\n",
    "\n",
    "3. **Chat-Logik & Kontext (Chat History)**\n",
    "   - Bei Chat-APIs werden Inputs oft als **Liste von Messages** gespeichert (z.B. `system` + `user`).\n",
    "   - Vorteil: Der Chat kann sich an Vorheriges erinnern → spätere Antworten werden durch den Verlauf beeinflusst.\n",
    "   - Prüfungs-typisch: erklären, warum eine System-Instruktion den Stil/Output ändert.\n",
    "\n",
    "4. **Rolle im Data-Science-Kontext**\n",
    "   - LLMs sind **Hilfswerkzeuge** (z.B. Erklären, Strukturieren, Zusammenfassen, Ideen generieren).\n",
    "   - Aber: Ergebnisse müssen **kritisch geprüft** werden (Plausibilität, Quellen, Logik) – kein „blindes Copy-Paste“.\n",
    "   - Prüfungs-typisch: kurze Reflexion „Mensch steuert – Modell unterstützt“.\n",
    "\n",
    "5. **Code lesen & erklären können (statt alles auswendig tippen)**\n",
    "   - Du solltest zu jedem Block sagen können: **Was passiert? Warum? Wo im Prozess?**\n",
    "   - Typische Stolpersteine:  \n",
    "     - Unterschied **Chat-Session** vs. **einzelner Generate-Call**  \n",
    "     - Unterschied **Konfiguration/Rolle** vs. **eigentliche Nutzerfrage**  \n",
    "     - Unterschied **Response-Objekt** vs. **Response-Text**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
